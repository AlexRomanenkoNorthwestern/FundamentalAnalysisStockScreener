#import Semrush
#import Finviz
#import Google
#import FootTraffic
import Yahoo
#import Dashboard
#import General
# import social media follower data

import pandas as pd
import datetime as dt
from datetime import datetime, timedelta, date, time
import numpy as np
import math


# Weights sum to 1
def weights (length):
    most_recent_weighting = .1
    weight_factor = .95
    vector = []
    for i in range(1,5):
        vector.append(most_recent_weighting)
        most_recent_weighting *= weight_factor
    total = sum(vector)
    array = np.array(vector)
    vector = array * 1/total #vector weights sum to 1
    return(vector)

# Weighted Mean
def m(x,w):
    total = 0
    x = np.array(x)
    for i in np.linspace(0,len(x)-1, num = len(x)):
        total += x[int(i)] * w[int(i)]
    return(total/np.sum(w))

# Weighted Covariance
def cov(x,y,w):
    total = 0
    x = np.array(x)
    y = np.array(y)
    for i in np.linspace(0,len(x)-1, num = len(x)):
        total += w[int(i)] * (x[int(i)] - m(x,w)) * (y[int(i)]- m(y,w))
    return(total/np.sum(w))

# Weighted Correlation
def weighted_corr(x,y,w):
    return(cov(x,y,w)/np.sqrt(cov(x,x,w)*cov(y,y,w)))


# Correlation between earnings estimates and historic eps
def correlation_estimates(ticker):
    yahoo_request = Yahoo.make_yahoo_request(ticker)
    historic_actual_eps = (Yahoo.get_historic_eps(yahoo_request, ticker))[0:4]
    historic_estimated_eps = Yahoo.get_historic_estimated_eps(yahoo_request)[0:4]
    print(historic_estimated_eps)
    print(historic_actual_eps)
    correlation = weighted_corr(historic_actual_eps, historic_estimated_eps, weights(4))
    return correlation

